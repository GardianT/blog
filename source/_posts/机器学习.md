---
title: 机器学习
date: 2020-12-10 17:41:30
tags:
---

[LR逻辑回归](https://zhuanlan.zhihu.com/p/61032148) 
机器学习基础流程：定义模型（函数），定义损失函数，调整特征，样本等使结果趋于正确  
lr模型函数fx = wx + b, 区间为实数集，最终需要预测概率，sigmod函数用于将fx最终结果区间归到01区间。  
损失函数用于指导模型如何训练出来w，或者按照什么方法训练出w。其实就是定义了预测结果与真实值的loss是多少。  
交叉熵是损失函数的一种。 [交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)  

梯度下降函数就是损失函数的偏导。即 &loss/w （&是阿尔法，打不出来）  
w的更新就是用上一次训练值减去下一次的偏导。w = w0 - &loss/w    

一个三层网络的深度学习模型，比如第一层函数f1x，第二层函数f2x，第三层函数f3x，偏导数为f1/x1, f2/x2, f3/x3  
如果出现梯度消失的情况，会使最终训练的w不再更新，如果出现梯度爆炸，偏导会变得不确定，训练出来的w会跳变，不收敛。  
 
[LR基础](https://www.cnblogs.com/sparkwen/p/3441197.html)算auc的方法，注意auc只判断相对顺序的正确性，不计算绝对值 

[softmax和sigmoid](https://www.zhihu.com/question/295247085/answer/974891555) 目前工程用softmax做归一化更多，因为softmax在k=2时候可以退化为sigmoid，多分类扩展性更好。常见word2vec等都用softmax归一化  

 
