---
title: 机器学习
date: 2020-12-10 17:41:30
tags:
---

[LR逻辑回归](https://zhuanlan.zhihu.com/p/61032148) 
机器学习基础流程：定义模型（函数），定义损失函数，调整特征，样本等使结果趋于正确  
lr模型函数fx = wx + b, 区间为实数集，最终需要预测概率，sigmod函数用于将fx最终结果区间归到01区间。  
损失函数用于指导模型如何训练出来w，或者按照什么方法训练出w。其实就是定义了预测结果与真实值的loss是多少。  
交叉熵是损失函数的一种。 [交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)  

梯度下降函数就是损失函数的偏导。即 &loss/w （&是阿尔法，打不出来）  
w的更新就是用上一次训练值减去下一次的偏导。w = w0 - &loss/w    

一个三层网络的深度学习模型，比如第一层函数f1x，第二层函数f2x，第三层函数f3x，偏导数为f1/x1, f2/x2, f3/x3  
如果出现梯度消失的情况，会使最终训练的w不再更新，如果出现梯度爆炸，偏导会变得不确定，训练出来的w会跳变，不收敛。  
 
[LR基础](https://www.cnblogs.com/sparkwen/p/3441197.html)算auc的方法，注意auc只判断相对顺序的正确性，不计算绝对值 

[softmax和sigmoid](https://www.zhihu.com/question/295247085/answer/974891555) 目前工程用softmax做归一化更多，因为softmax在k=2时候可以退化为sigmoid，多分类扩展性更好。常见word2vec等都用softmax归一化  

[tensorflow如何注册和调用op的](https://zhuanlan.zhihu.com/p/34168765)  
[tensorflow源码解析](https://blog.csdn.net/u013510838/article/details/84103503) 


协同过滤的向量生成：计算i2i的相似度，比如抽取10万个user，每个user固定位置，买过这个品就是1，没买过这个品就是0。这样每个品就可以产生10w维的向量，也可以计算i2i相似度了  
word2vec：针对user seq进行分词，每个词都会初始化一个向量出来（模型实现的，不一定是全0，初始化的分布满足正态分布），在网络里进行学习，词之间可以进行更新，比如【w1，w2，w3】，w2经常出现在w1，w3中间，这样w1和w3就会更新w2的向量。其他词同理  
双塔向量的生成：与word2vec基本同理，初始化向量，但是用的是用户特征，item特征等训练，结合loss函数等进行更新。 


协同过滤缺点：  
1. 无行为无法预测：冷启动  
2. 行为丰富：哈利波特问题。比如iphone可能所有用户都买，在协同过滤场景，跟每个item都产生了关联，这样推荐时候会给每个用户都推荐
推荐系统除了推荐准确性以外，还要考虑新颖性，丰富性问题。准确性可以通过准召率来评估，新颖可以通过多路召回等来弥补。user cf比item cf新颖性更好，但是准确性弱。i2i工程难度小，实时性强，是match常用算法。 
